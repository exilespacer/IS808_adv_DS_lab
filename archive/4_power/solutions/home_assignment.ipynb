{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DAO Power Analysis\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from gql import gql, Client\n",
    "from gql.transport.aiohttp import AIOHTTPTransport\n",
    "\n",
    "from etherscan import Etherscan\n",
    "\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "\n",
    "import scipy.stats as st\n",
    "\n",
    "\n",
    "print(pd.__version__)\n",
    "print(np.__version__)\n",
    "print(requests.__version__)\n",
    "print(plt.matplotlib.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import authentication key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://martin-thoma.com/configuration-files-in-python/\n",
    "\n",
    "import json\n",
    "\n",
    "with open(\"../3_api/.private/keys.json\") as keys_file:\n",
    "    KEYS = json.load(keys_file)\n",
    "\n",
    "# Note: don't print the key, or if you do, delete the cell's output\n",
    "# (cell outputs are saved and can be sent to Github)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define functions to call different APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DEEPDAO\n",
    "\n",
    "\n",
    "def deepdao(query, params=None, post=False):\n",
    "\n",
    "    ENDPOINT = \"https://api.deepdao.io/v0.1/\"\n",
    "\n",
    "    headers = {\"x-api-key\": KEYS[\"DEEPDAO\"], \"accept\": \"application/json\"}\n",
    "\n",
    "    if post:\n",
    "        response = requests.post(ENDPOINT + query, headers=headers, json=params)\n",
    "    else:\n",
    "        response = requests.get(ENDPOINT + query, headers=headers, params=params)\n",
    "\n",
    "    print(response)\n",
    "    return response.json()\n",
    "\n",
    "\n",
    "## ETHERSCAN\n",
    "############\n",
    "\n",
    "\n",
    "def etherscan(params={}):\n",
    "\n",
    "    ENDPOINT = \"https://api.etherscan.io/api\"\n",
    "\n",
    "    params[\"apikey\"] = KEYS[\"ETHERSCAN\"]\n",
    "\n",
    "    response = requests.get(\n",
    "        ENDPOINT,\n",
    "        headers={\"accept\": \"application/json\", \"User-Agent\": \"\"},\n",
    "        params=params,\n",
    "    )\n",
    "\n",
    "    print(response)\n",
    "    return response.json()\n",
    "\n",
    "\n",
    "eth = Etherscan(KEYS[\"ETHERSCAN\"])\n",
    "\n",
    "## SNAPSHOT\n",
    "###########\n",
    "\n",
    "SNAPSHOT_ENDPOINT = \"https://hub.snapshot.org/graphql\"\n",
    "\n",
    "snapshot = Client(transport=AIOHTTPTransport(url=SNAPSHOT_ENDPOINT))\n",
    "\n",
    "\n",
    "def snapshot_rest(query, params=None):\n",
    "\n",
    "    response = requests.post(\n",
    "        SNAPSHOT_ENDPOINT,\n",
    "        headers={\"accept\": \"application/json\"},\n",
    "        params={\"query\": query},\n",
    "    )\n",
    "\n",
    "    print(response)\n",
    "    return response.json()[\"data\"]\n",
    "\n",
    "\n",
    "## THE GRAPH\n",
    "############\n",
    "\n",
    "## Endpoints depends on subgraph of interest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load DAOs.\n",
    "\n",
    "Saved in previous lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## All Daos according to Deepdao\n",
    "daos_deepdao = pd.read_json(\"../3_api/deepdao/data/daos_deepdao.json\")\n",
    "daos_deepdao.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Warning: it throws an error.\n",
    "\n",
    "## All Daos according to Snapshot\n",
    "daos_snapshot = pd.read_json(\"../3_api/snapshot/data/daos_snapshot.json\")\n",
    "daos_snapshot.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Workaround."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../3_api/snapshot/data/daos_snapshot.json\") as file:\n",
    "    daos_snapshot = json.load(file)\n",
    "\n",
    "daos_snapshot = pd.DataFrame(daos_snapshot)\n",
    "daos_snapshot.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updated Utility Functions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load JSON files into Pandas.\n",
    "\n",
    "Pandas throws an error if JSON contains too large ints, so we load JSON separately and then feed the Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pd_read_json(file):\n",
    "    ## Prevents Value too big Error.\n",
    "    with open(file) as f:\n",
    "        df = json.load(f)\n",
    "    df = pd.DataFrame(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keep GQL queries separated.\n",
    "\n",
    "GQL queries can be quite cumbersome, so it's better to write them into a separate file and then load them in here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_query(filename, do_gql=False):\n",
    "    with open(\"gql_queries/\" + filename.replace(\".gql\", \"\") + \".gql\") as f:\n",
    "        query = f.read()\n",
    "        if do_gql:\n",
    "            query = gql(query)\n",
    "    return query\n",
    "\n",
    "\n",
    "## Alias gq.\n",
    "gq = get_query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Enhanced API Wrapper\n",
    "\n",
    "With this wrapper we want to:\n",
    "- Save data as we download it in order to avoid data loss in case of an error (remote or local).\n",
    "- Save the data periodically and in small chuncks, which is usually easier to load back.\n",
    "- Catch errors as much as possible, to terminate gracefully the execution.\n",
    "- Execute both GQL and simple rest requests for any type of query.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def gql_all(\n",
    "    query,\n",
    "    field,\n",
    "    first=1000,\n",
    "    skip=None,\n",
    "    initial_list=None,\n",
    "    counter=True,\n",
    "    limit=None,\n",
    "    save=None,\n",
    "    save_interval=10,\n",
    "    clear_on_save=False,\n",
    "    append=True,\n",
    "    rest=False,\n",
    "    data_dir=\"data\",\n",
    "    save_counter=1,\n",
    "    vars=None,\n",
    "):\n",
    "\n",
    "    ## The returned value and the varible used to accumulate results.\n",
    "    out = []\n",
    "\n",
    "    ## Utility function to save intermediate and final results.\n",
    "    def save_json():\n",
    "\n",
    "        # Pandas has problem load pure json saves.\n",
    "        # Hence we create a pandas Dataframe and save it.\n",
    "        # nonlocal append\n",
    "        # flag = \"a\" if append else \"w\"\n",
    "        # with open(\"data/\" + save, flag) as f:\n",
    "        #     json.dump(out, f)\n",
    "        #     print(\"Saved.\")\n",
    "\n",
    "        nonlocal out\n",
    "        df = pd.DataFrame(out)\n",
    "\n",
    "        if clear_on_save:\n",
    "\n",
    "            nonlocal save_counter\n",
    "\n",
    "            sv = str(save_counter)\n",
    "            sv = sv.zfill(5)\n",
    "            save_counter += 1\n",
    "\n",
    "            filename = save.replace(\".json\", \"_\" + sv + \".json\")\n",
    "\n",
    "            out = []\n",
    "            out_str = \"Saved and cleared.\"\n",
    "        else:\n",
    "            filename = save\n",
    "            out_str = \"Saved.\"\n",
    "\n",
    "        df.to_json(data_dir + \"/\" + filename, orient=\"records\")\n",
    "        print(out_str)\n",
    "\n",
    "    ## Load initial list.\n",
    "    ## If no skip is provided, then skip is set to the length of\n",
    "    ## the initial list, otherwise we use the user-specified value\n",
    "    if initial_list:\n",
    "        out = initial_list\n",
    "        if skip is None:\n",
    "            skip = len(out)\n",
    "    elif skip is None:\n",
    "        skip = 0\n",
    "\n",
    "    ## Make a GQL query object, if necessary.\n",
    "    if not rest and type(query) == str:\n",
    "        query = gql(query)\n",
    "\n",
    "    my_counter = 0\n",
    "    fetch = True\n",
    "    try:\n",
    "        while fetch:\n",
    "\n",
    "            my_counter += 1\n",
    "            if limit and my_counter > limit:\n",
    "                print(\"**Limit reached: \", limit)\n",
    "                fetch = False\n",
    "                continue\n",
    "\n",
    "            if rest:\n",
    "\n",
    "                # Building query manually.\n",
    "                q = query.replace(\"($first: Int!, $skip: Int!)\", \"\")\n",
    "                q = q.replace(\"$first\", str(first))\n",
    "                q = q.replace(\"$skip\", str(skip))\n",
    "                # print(q)\n",
    "\n",
    "                ## Optional additional variables.\n",
    "                if vars:\n",
    "                    for v in vars:\n",
    "                        q = q.replace(\"$\" + v, str(vars[v]))\n",
    "\n",
    "                res = snapshot_rest(q)\n",
    "\n",
    "            else:\n",
    "\n",
    "                _vars = {\"first\": first, \"skip\": skip}\n",
    "\n",
    "                ## Optional additional variables.\n",
    "                if vars:\n",
    "                    _vars = _vars | vars\n",
    "\n",
    "                res = await snapshot.execute_async(query, variable_values=_vars)\n",
    "\n",
    "            if not res[field]:\n",
    "                print(\"**I am done fetching!**\")\n",
    "                fetch = False\n",
    "            else:\n",
    "                out.extend(res[field])\n",
    "                skip += first\n",
    "                if counter:\n",
    "                    print(my_counter, len(out))\n",
    "\n",
    "                if save and my_counter % save_interval == 0:\n",
    "                    save_json()\n",
    "\n",
    "        if save and my_counter % save_interval != 0:\n",
    "            save_json()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "        print(\"**An error occurred, exiting early.**\")\n",
    "        if save:\n",
    "            save_json()\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we split data from the API in multiple smaller files, we want to have a quick method to load them back into one place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pd_read_dir(dir, blacklist=None, whitelist=None, ext=(\".json\")):\n",
    "    dir_df = pd.DataFrame()\n",
    "\n",
    "    for file in os.listdir(dir):\n",
    "        if blacklist and file in blacklist:\n",
    "            continue\n",
    "        if whitelist and file not in whitelist:\n",
    "            continue\n",
    "\n",
    "        if file.endswith(ext):\n",
    "            tmp_df = pd_read_json(dir + \"/\" + file)\n",
    "            dir_df = pd.concat([dir_df, tmp_df])\n",
    "\n",
    "    return dir_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We did not fetch all the useful information in the previous lecture. Let's do it again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise: use the functions above to load all spaces**\n",
    "\n",
    "Important! Try with GQL and with REST.\n",
    "\n",
    "Remember to save the file.\n",
    "\n",
    "Try different values of `first`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spaces_query = gq(\"snapshot_spaces\")\n",
    "\n",
    "res = await gql_all(\n",
    "    spaces_query,\n",
    "    field=\"spaces\",\n",
    "    # rest=True,\n",
    "    save=\"snapshot_spaces.json\",\n",
    "    data_dir=\"data/\",\n",
    "    first=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise: verify that you can import the file containing the spaces that just saved**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spaces = pd_read_json(\"data/snapshot_spaces.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise: plot the relationship between followers and proposals across DAOs**\n",
    "\n",
    "Question: ...and votes?\n",
    "Bonus: jitter the dots, and try log scales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spaces = pd.DataFrame(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spaces.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spaces.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jitter(x):\n",
    "    x = x + random.uniform(0, 0.5) - 0.25\n",
    "    if x <= 1:\n",
    "        x = 0\n",
    "    return x\n",
    "\n",
    "\n",
    "spaces[\"followersCount_j\"] = spaces[\"followersCount\"].apply(lambda x: jitter(x))\n",
    "spaces[\"proposalsCount_j\"] = spaces[\"proposalsCount\"].apply(lambda x: jitter(x))\n",
    "\n",
    "\n",
    "spaces.plot.scatter(\n",
    "    \"followersCount_j\", \"proposalsCount_j\", figsize=(10, 10), loglog=True\n",
    ")  # logy=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking last week home assignment.\n",
    "\n",
    "- Pick a small (<1k), medium (<10k), and large  DAO (10k+)\n",
    "- Get all members (if possible try different APIs)\n",
    "- Order them by wealth in ETH\n",
    "- Compute correlation ETH wealth and number of votes in the DAO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see whether our cutpoints make sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dao_size(x, T1=1000, T2=10000):\n",
    "    if x < T1:\n",
    "        return \"small\"\n",
    "    if x < T2:\n",
    "        return \"medium\"\n",
    "    return \"large\"\n",
    "\n",
    "\n",
    "spaces[\"size\"] = spaces[\"followersCount\"].apply(dao_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spaces[\"size\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the DAOs are rather small in size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spaces[\"followersCount\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spaces10 = spaces[spaces[\"followersCount\"] >= 10]\n",
    "spaces10.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = spaces10[spaces10[\"size\"] == \"small\"].sample(1)\n",
    "sm[[\"name\", \"followersCount\", \"website\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## id = hashflowdao.eth\n",
    "DAO_ID = sm[\"id\"].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_followers = gq(\"snapshot_followers_of_space\")\n",
    "followers = await gql_all(\n",
    "    query_followers,\n",
    "    field=\"follows\",\n",
    "    # rest=True,\n",
    "    # save=\"snapshot_followers_small.json\",\n",
    "    data_dir=\"data/\",\n",
    "    first=1000,\n",
    "    vars={\"space\": DAO_ID},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(followers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "followers = pd.DataFrame(res[\"follows\"])\n",
    "followers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sum(followers.duplicated()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to get the **wealth** in ETH of the followers.\n",
    "\n",
    "Etherscan's api is quite handy...\n",
    "\n",
    "**Exercise: finish the function below**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eth_wealth(addresses, K=20, limit=None):\n",
    "\n",
    "    idx = 0\n",
    "    counter = 0\n",
    "    eth_wealth = []\n",
    "    n_addresses = len(addresses)\n",
    "\n",
    "    while (idx < n_addresses) and (limit is None or counter < limit):\n",
    "        _addresses = addresses[idx : min(idx + K, n_addresses)]\n",
    "        # print(addresses)\n",
    "        w = eth.get_eth_balance_multiple(_addresses)\n",
    "        eth_wealth += w\n",
    "        ## Update indexes.\n",
    "        counter += 1\n",
    "        idx += K\n",
    "        print(counter, idx, len(eth_wealth))\n",
    "\n",
    "    print(\"**Got all of them!\")\n",
    "    return eth_wealth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eth_wealth = get_eth_wealth(followers[\"follower\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_wealth = pd.DataFrame(eth_wealth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_wealth.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the votes of the space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_votes = gq(\"snapshot_votes_of_space\")\n",
    "votes = await gql_all(\n",
    "    query_votes,\n",
    "    field=\"votes\",\n",
    "    # rest=True,\n",
    "    # save=\"snapshot_followers_small.json\",\n",
    "    data_dir=\"data/\",\n",
    "    first=1000,\n",
    "    vars={\"space\": DAO_ID},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "votes = pd.DataFrame(votes)\n",
    "votes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise: Unnest proposal id.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "votes[\"proposal\"] = votes[\"proposal\"].apply(lambda x: x[\"id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "votes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "votes[\"proposal\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "votes[\"voter\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "votes[\"choice\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have more votes than followers...\\\n",
    "After inquirying with Snapshot staff on Discord, **it is possible to vote without following an organization.**\\\n",
    "Let's get the wealth of all voters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eth_wealth2 = get_eth_wealth(votes[\"voter\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eth_wealth2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_wealth = pd.DataFrame(eth_wealth2)\n",
    "sm_wealth.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "votes_wealth = pd.merge(sm_wealth, votes, left_on=\"account\", right_on=\"voter\")\n",
    "votes_wealth[\"balance\"] = votes_wealth[\"balance\"].astype(\"float64\")\n",
    "votes_wealth.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_groups = votes_wealth.groupby(\"choice\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_groups[\"balance\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = df_groups[\"balance\"].agg([\"mean\", \"sem\"])\n",
    "stats[\"ci95_hi\"] = stats[\"mean\"] + 1.96 * stats[\"sem\"]\n",
    "stats[\"ci95_lo\"] = stats[\"mean\"] - 1.96 * stats[\"sem\"]\n",
    "print(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Equivalent to:\n",
    "\n",
    "ci = st.norm.interval(confidence=0.95, loc=stats[\"mean\"], scale=stats[\"sem\"])\n",
    "\n",
    "stats[\"ci95_lo_b\"] = ci[0]\n",
    "stats[\"ci95_hi_b\"] = ci[1]\n",
    "\n",
    "print(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=3, nrows=1)\n",
    "\n",
    "for c, ax in enumerate(axes.flatten()):\n",
    "    c += 1\n",
    "    x = votes_wealth[votes_wealth[\"choice\"] == c]\n",
    "    ax.boxplot(x[\"balance\"])\n",
    "\n",
    "    ax.set_title(\"Choice={}\".format(c))\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Messages\n",
    "\n",
    "We get a better idea about the history of our DAO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = gq(\"snapshot_messages_of_space\")\n",
    "messages = await gql_all(\n",
    "    query,\n",
    "    field=\"messages\",\n",
    "    # rest=True,\n",
    "    # save=\"snapshot_followers_small.json\",\n",
    "    data_dir=\"data/\",\n",
    "    first=1000,\n",
    "    vars={\"space\": DAO_ID},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = pd.DataFrame(messages)\n",
    "messages.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages[\"type\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of votes is larger than what we downloaded and the number of follows smaller. After inquiring with the Snapshot's staff on Discord:\n",
    "\n",
    "- admins might have deleted/closed/archived some proposals,\n",
    "- one should check the data of creation of a dato, as some features of the platform might have been added later. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "c32e70da788b7e7a251586b987d2c569d373765eb2af3ced0674c402bf99c5a2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
