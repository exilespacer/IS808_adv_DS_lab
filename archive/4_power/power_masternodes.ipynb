{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Power Masternodes\n",
    "\n",
    "First, load everthing at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from gql import gql, Client\n",
    "from gql.transport.aiohttp import AIOHTTPTransport\n",
    "\n",
    "from etherscan import Etherscan\n",
    "\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "\n",
    "import scipy.stats as st\n",
    "\n",
    "\n",
    "print(pd.__version__)\n",
    "print(np.__version__)\n",
    "print(requests.__version__)\n",
    "print(plt.matplotlib.__version__)\n",
    "\n",
    "\n",
    "# https://martin-thoma.com/configuration-files-in-python/\n",
    "\n",
    "import json\n",
    "\n",
    "with open(\"../3_api/.private/keys.json\") as keys_file:\n",
    "    KEYS = json.load(keys_file)\n",
    "\n",
    "# Note: don't print the key, or if you do, delete the cell's output\n",
    "# (cell outputs are saved and can be sent to Github).\n",
    "\n",
    "\n",
    "## DEEPDAO\n",
    "\n",
    "\n",
    "def deepdao(query, params=None, post=False):\n",
    "\n",
    "    ENDPOINT = \"https://api.deepdao.io/v0.1/\"\n",
    "\n",
    "    headers = {\"x-api-key\": KEYS[\"DEEPDAO\"], \"accept\": \"application/json\"}\n",
    "\n",
    "    if post:\n",
    "        response = requests.post(ENDPOINT + query, headers=headers, json=params)\n",
    "    else:\n",
    "        response = requests.get(ENDPOINT + query, headers=headers, params=params)\n",
    "\n",
    "    print(response)\n",
    "    return response.json()\n",
    "\n",
    "\n",
    "## ETHERSCAN\n",
    "############\n",
    "\n",
    "\n",
    "def etherscan(params={}):\n",
    "\n",
    "    ENDPOINT = \"https://api.etherscan.io/api\"\n",
    "\n",
    "    params[\"apikey\"] = KEYS[\"ETHERSCAN\"]\n",
    "\n",
    "    response = requests.get(\n",
    "        ENDPOINT,\n",
    "        headers={\"accept\": \"application/json\", \"User-Agent\": \"\"},\n",
    "        params=params,\n",
    "    )\n",
    "\n",
    "    print(response)\n",
    "    return response.json()\n",
    "\n",
    "\n",
    "eth = Etherscan(KEYS[\"ETHERSCAN\"])\n",
    "\n",
    "## SNAPSHOT\n",
    "###########\n",
    "\n",
    "SNAPSHOT_ENDPOINT = \"https://hub.snapshot.org/graphql\"\n",
    "\n",
    "snapshot = Client(transport=AIOHTTPTransport(url=SNAPSHOT_ENDPOINT))\n",
    "\n",
    "\n",
    "def snapshot_rest(query, params=None):\n",
    "\n",
    "    response = requests.post(\n",
    "        SNAPSHOT_ENDPOINT,\n",
    "        headers={\"accept\": \"application/json\"},\n",
    "        params={\"query\": query},\n",
    "    )\n",
    "\n",
    "    print(response)\n",
    "    return response.json()[\"data\"]\n",
    "\n",
    "\n",
    "## THE GRAPH\n",
    "############\n",
    "\n",
    "## Endpoints depends on subgraph of interest.\n",
    "\n",
    "\n",
    "def pd_read_json(file):\n",
    "    ## Prevents Value too big Error.\n",
    "    with open(file) as f:\n",
    "        df = json.load(f)\n",
    "    df = pd.DataFrame(df)\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_query(filename, do_gql=False):\n",
    "    with open(\"gql_queries/\" + filename.replace(\".gql\", \"\") + \".gql\") as f:\n",
    "        query = f.read()\n",
    "        if do_gql:\n",
    "            query = gql(query)\n",
    "    return query\n",
    "\n",
    "\n",
    "## Alias gq.\n",
    "gq = get_query\n",
    "\n",
    "\n",
    "def get_query(filename, do_gql=False):\n",
    "    with open(\"gql_queries/\" + filename.replace(\".gql\", \"\") + \".gql\") as f:\n",
    "        query = f.read()\n",
    "        if do_gql:\n",
    "            query = gql(query)\n",
    "    return query\n",
    "\n",
    "\n",
    "## Alias gq.\n",
    "gq = get_query\n",
    "\n",
    "\n",
    "async def gql_all(\n",
    "    query,\n",
    "    field,\n",
    "    first=1000,\n",
    "    skip=None,\n",
    "    initial_list=None,\n",
    "    counter=True,\n",
    "    limit=None,\n",
    "    save=None,\n",
    "    save_interval=10,\n",
    "    clear_on_save=False,\n",
    "    append=True,\n",
    "    rest=False,\n",
    "    data_dir=\"data\",\n",
    "    save_counter=1,\n",
    "    vars=None,\n",
    "):\n",
    "\n",
    "    ## The returned value and the varible used to accumulate results.\n",
    "    out = []\n",
    "\n",
    "    ## Utility function to save intermediate and final results.\n",
    "    def save_json():\n",
    "\n",
    "        # Pandas has problem load pure json saves.\n",
    "        # Hence we create a pandas Dataframe and save it.\n",
    "        # nonlocal append\n",
    "        # flag = \"a\" if append else \"w\"\n",
    "        # with open(\"data/\" + save, flag) as f:\n",
    "        #     json.dump(out, f)\n",
    "        #     print(\"Saved.\")\n",
    "\n",
    "        nonlocal out\n",
    "        df = pd.DataFrame(out)\n",
    "\n",
    "        if clear_on_save:\n",
    "\n",
    "            nonlocal save_counter\n",
    "\n",
    "            sv = str(save_counter)\n",
    "            sv = sv.zfill(5)\n",
    "            save_counter += 1\n",
    "\n",
    "            filename = save.replace(\".json\", \"_\" + sv + \".json\")\n",
    "\n",
    "            out = []\n",
    "            out_str = \"Saved and cleared.\"\n",
    "        else:\n",
    "            filename = save\n",
    "            out_str = \"Saved.\"\n",
    "\n",
    "        df.to_json(data_dir + \"/\" + filename, orient=\"records\")\n",
    "        print(out_str)\n",
    "\n",
    "    ## Load initial list.\n",
    "    ## If no skip is provided, then skip is set to the length of\n",
    "    ## the initial list, otherwise we use the user-specified value\n",
    "    if initial_list:\n",
    "        out = initial_list\n",
    "        if skip is None:\n",
    "            skip = len(out)\n",
    "    elif skip is None:\n",
    "        skip = 0\n",
    "\n",
    "    ## Make a GQL query object, if necessary.\n",
    "    if not rest and type(query) == str:\n",
    "        query = gql(query)\n",
    "\n",
    "    my_counter = 0\n",
    "    fetch = True\n",
    "    try:\n",
    "        while fetch:\n",
    "\n",
    "            my_counter += 1\n",
    "            if limit and my_counter > limit:\n",
    "                print(\"**Limit reached: \", limit)\n",
    "                fetch = False\n",
    "                continue\n",
    "\n",
    "            if rest:\n",
    "\n",
    "                # Building query manually.\n",
    "                q = query.replace(\"($first: Int!, $skip: Int!)\", \"\")\n",
    "                q = q.replace(\"$first\", str(first))\n",
    "                q = q.replace(\"$skip\", str(skip))\n",
    "                # print(q)\n",
    "\n",
    "                ## Optional additional variables.\n",
    "                if vars:\n",
    "                    for v in vars:\n",
    "                        q = q.replace(\"$\" + v, str(vars[v]))\n",
    "\n",
    "                res = snapshot_rest(q)\n",
    "\n",
    "            else:\n",
    "\n",
    "                _vars = {\"first\": first, \"skip\": skip}\n",
    "\n",
    "                ## Optional additional variables.\n",
    "                if vars:\n",
    "                    _vars = _vars | vars\n",
    "\n",
    "                res = await snapshot.execute_async(query, variable_values=_vars)\n",
    "\n",
    "            if not res[field]:\n",
    "                print(\"**I am done fetching!**\")\n",
    "                fetch = False\n",
    "            else:\n",
    "                out.extend(res[field])\n",
    "                skip += first\n",
    "                if counter:\n",
    "                    print(my_counter, len(out))\n",
    "\n",
    "                if save and my_counter % save_interval == 0:\n",
    "                    save_json()\n",
    "\n",
    "        if save and my_counter % save_interval != 0:\n",
    "            save_json()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "        print(\"**An error occurred, exiting early.**\")\n",
    "        if save:\n",
    "            save_json()\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def pd_read_dir(dir, blacklist=None, whitelist=None, ext=(\".json\")):\n",
    "    dir_df = pd.DataFrame()\n",
    "\n",
    "    for file in os.listdir(dir):\n",
    "        if blacklist and file in blacklist:\n",
    "            continue\n",
    "        if whitelist and file not in whitelist:\n",
    "            continue\n",
    "\n",
    "        if file.endswith(ext):\n",
    "            tmp_df = pd_read_json(dir + \"/\" + file)\n",
    "            dir_df = pd.concat([dir_df, tmp_df])\n",
    "\n",
    "    return dir_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing to compute power as in Mosley et al. (2022).\n",
    "\n",
    "\"Towards a systemic understanding of blockchain governance in proposal voting: A dash case study.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load `spaces`, `proposals`, and `votes`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spaces = pd_read_json(\"data/snapshot_spaces.json\")\n",
    "\n",
    "all_proposals = pd_read_json(\"data/5_snapshot_pancake_proposals.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## If downloaded already.\n",
    "all_votes = pd_read_dir(\"data/votes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Otherwise.\n",
    "## This query takes a while...\n",
    "# votes_query = gq(\"snapshot_votes\")\n",
    "# res = await gql_all(votes_query,\n",
    "#                     field=\"votes\",\n",
    "#                     rest=True,\n",
    "#                     save=\"snapshot_votes_test.json\",\n",
    "#                     data_dir=\"data/votes/\",\n",
    "#                     save_interval = 20,\n",
    "#                     limit=2,\n",
    "#                     first=20000, # First can be a high number.\n",
    "#                     clear_on_save=True\n",
    "#                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"spaces: \", len(spaces))\n",
    "print(\"proposals: \", len(all_proposals))\n",
    "print(\"votes: \", len(all_votes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleanup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Proposals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_proposals.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_proposals['space'] = all_proposals['space'].apply(lambda x : x['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_proposals[\"space\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Votes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_votes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_votes['space'] = all_votes['space'].apply(lambda x : x['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Returns an error, we need to account for a None field.\n",
    "# # all_votes['proposal'] = all_votes['proposal'].apply(lambda x : x['id'])\n",
    "\n",
    "# all_votes['proposal'] = all_votes['proposal'].apply(lambda x :\n",
    "#     x if x is None else x['id']\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pancake Swap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Who did most of the proposals?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_props = spaces[spaces[\"proposalsCount\"] == max(spaces[\"proposalsCount\"])]\n",
    "DAO_MOST_PROPS_ID = most_props[\"id\"].iloc[0]\n",
    "DAO_MOST_PROPS_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_proposals[\"space\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pancake_props = all_proposals[all_proposals[\"space\"] == DAO_MOST_PROPS_ID]\n",
    "pancake_props.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pancake_votes = all_votes[all_votes[\"space\"] == DAO_MOST_PROPS_ID]\n",
    "pancake_votes.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate an error, there are mixed types.\n",
    "pancake_votes[\"choice\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pancake_votes[\"choice\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's remove non 'int' votes (e.g., ranked choices).\n",
    "\n",
    "In the real analysis we should try to analyze all data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(pancake_votes))\n",
    "pancake_votes = pancake_votes[pancake_votes[\"choice\"].isin([1, 2, 3])]\n",
    "print(len(pancake_votes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pancake_votes[\"choice\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's center them around zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pancake_votes[\"choice\"] = pancake_votes[\"choice\"] - 2\n",
    "pancake_votes[\"choice\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How much a node deviate from the others in every proposal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclid(row):\n",
    "    vote = row[\"choice\"]\n",
    "    proposal = row[\"proposal\"]\n",
    "    other_votes = pancake_votes[pancake_votes[\"proposal\"] == proposal]\n",
    "    distances = other_votes[\"choice\"].apply(lambda x: math.pow((vote - x), 2))\n",
    "    return math.sqrt(sum(distances))\n",
    "\n",
    "\n",
    "pancake_votes[\"vote_distance\"] = pancake_votes.apply(euclid, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pancake_votes[\"vote_distance\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pancake_votes[\"vote_distance\"].plot.hist(bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pancake_prop_groups = pancake_votes.groupby(\"proposal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the proposals with the highest variation in voting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pancake_prop_groups[\"vote_distance\"].describe().sort_values(\"mean\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise: implement the Masternode Voting Network algorithm**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voters = pancake_votes[\"voter\"].unique()\n",
    "len(voters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
